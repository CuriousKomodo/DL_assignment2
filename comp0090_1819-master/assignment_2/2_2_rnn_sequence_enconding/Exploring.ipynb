{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/kefei/Documents/DSML/Introduction to Deep Learning/Second assignment/comp0090_1819-master/assignment_2/2_2_rnn_sequence_enconding/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    text_file = open(path, \"r\")\n",
    "    lines = text_file.readlines()\n",
    "    \n",
    "    x, y = [], []\n",
    "    for line in lines:\n",
    "        line_tokens = line.split(\" \")\n",
    "        y.append(int(line_tokens[0].replace(\"(\",\"\")))\n",
    "\n",
    "        word_token_list = []\n",
    "        for token in line_tokens:\n",
    "            if token.endswith(')'):\n",
    "                word_token_list.append(token.replace(')',''))\n",
    "        x.append(word_token_list)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y = read_data(path+\"train.sexp\");\n",
    "dev_x,   dev_y   = read_data(path+\"dev.sexp\");\n",
    "test_x,  test_y  = read_data(path+\"test.sexp\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "#                             tokenizer = callable,    \\\n",
    "#                             preprocessor = None, \\\n",
    "#                             stop_words = None) \n",
    "#train_data_features = vectorizer.fit_transform(train_x)\n",
    "#train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only consider the top 1000 most popular words. \n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False) #max_features=1000\n",
    "cbow=vectorizer.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = cbow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make a matrix that stores the indices of each sentence up to 30 words. \n",
    "embedding = 30\n",
    "index_matrix = np.zeros((len(train_x),embedding))\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(min(len(train_x[i]),embedding)):\n",
    "        word = train_x[i][j]\n",
    "        index_matrix[i,j] = vectorizer.vocabulary_[str(word)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = cbow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.zeros(len(train_x))\n",
    "for i in range(len(train_x)):\n",
    "    lengths[i] = len(train_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are expected to provide an analysis of the provided data. This can include word frequency analysis by class, character-level frequency analysis, analysis based on manual features (e.g. stopwords, negation words, punctuation), etc. Try to include some creative visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8672, 128)\n"
     ]
    }
   ],
   "source": [
    "parameters =initialize_parameters(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (8544,18382) and (8672,128) not aligned: 18382 (dim 1) != 8672 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-33389138d9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLSTM_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcbow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-227-e2d157ae4467>\u001b[0m in \u001b[0;36mLSTM_forward\u001b[0;34m(x, y, cbow, parameters, word_dict, index_matrix, batch_size, sentence_size)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mWf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mWi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (8544,18382) and (8672,128) not aligned: 18382 (dim 1) != 8672 (dim 0)"
     ]
    }
   ],
   "source": [
    "LSTM_network = LSTM_forward(x,y,cbow,parameters,word_dict,index_matrix,batch_size=None,sentence_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((4,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[,[2,3]]=1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Recurrent Neural Network (RNN) to encode the sequence and predict each of the 5 classes. Tune hyper-parameters on the dev set. Evaluate performance in terms of accuracy on the test set.Bonus question: Compare performance on the train/dev/test sets - what does this tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(x):\n",
    "    hidden_size = 128 # Number of LSTM layer's neurons, a,f,i,o\n",
    "    n = x.shape[0] # Number of input dimension == number of items in vocabulary\n",
    "    W_size = hidden_size + n # Because we will concatenate LSTM state with the input\n",
    "    \n",
    "    \n",
    "    #Maybe will concatenate Wf,Wi,Wc and Wo together to multiply in parallel\n",
    "    Wf = np.random.randn(W_size, hidden_size) / np.sqrt( W_size/ 2.)\n",
    "    Wi = np.random.randn(W_size, hidden_size) / np.sqrt( W_size/ 2.)\n",
    "    Wa = np.random.randn(W_size, hidden_size) / np.sqrt( W_size/ 2.)\n",
    "    Wo = np.random.randn(W_size, hidden_size) / np.sqrt( W_size/ 2.)\n",
    "    Wy = np.random.randn(hidden_size, n) / np.sqrt(n / 2.)\n",
    "    \n",
    "    print(Wf.shape)\n",
    "    \n",
    "    bf=np.zeros((1, hidden_size))\n",
    "    bi=np.zeros((1, hidden_size))\n",
    "    ba=np.zeros((1, hidden_size))\n",
    "    bo=np.zeros((1, hidden_size))\n",
    "    by=np.zeros((1, n))\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['Wf'] = Wf\n",
    "    parameters['bf'] = bf\n",
    "    parameters['Wi'] = Wi\n",
    "    parameters['bi'] = bi\n",
    "    parameters['Wa'] = Wa\n",
    "    parameters['ba'] = ba\n",
    "    parameters['Wo'] = Wo\n",
    "    parameters['bo'] = bo\n",
    "    parameters['Wy'] = Wy\n",
    "    parameters['by'] = by\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def initialize_network(hidden_size,T,batch_size):\n",
    "    f = np.zeros((T,batch_size,hidden_size))\n",
    "    i = np.zeros((T,batch_size,hidden_size))\n",
    "    a = np.zeros((T,batch_size,hidden_size))\n",
    "    o = np.zeros((T,batch_size,hidden_size))\n",
    "    c = np.zeros((T,batch_size,hidden_size))\n",
    "    h = np.zeros((T,batch_size,hidden_size))\n",
    "    \n",
    "    LSTM_network = {'f':f,'i':i,'a':a,'o':o,'c':c,'h':h}\n",
    "    return LSTM_network\n",
    "\n",
    "\n",
    "def initialize_grads(input_size,h_size):\n",
    "    d_Wa = np.zeros(input_size, h_size)\n",
    "    d_Wi = np.zeros(input_size, h_size)\n",
    "    d_Wf = np.zeros(input_size, h_size)\n",
    "    d_Wo = np.zeros(input_size, h_size)\n",
    "\n",
    "    d_ba = np.zeros(h_size)\n",
    "    d_bi = np.zeros(h_size)\n",
    "    d_bf = np.zeros(h_size)\n",
    "    d_bo = np.zeros(h_size)\n",
    "    \n",
    "    grads = {'d_Wa':d_Wa,'d_Wi':d_Wi,'d_Wf':d_Wf,'d_Wo':d_Wo,'d_ba':d_ba,'d_bi':d_bi,'d_bf':d_bf, 'd_bo':d_bo}\n",
    "    return grads\n",
    "\n",
    "\n",
    "#Input one current X or one sentence? \n",
    "#index matrix need to be adjusted with batch! \n",
    "def LSTM_forward(x,y,cbow,parameters,word_dict,index_matrix,batch_size=None,sentence_size=30):\n",
    "    hidden_size=128\n",
    "    D = cbow.shape[1]\n",
    "    if not batch_size:\n",
    "        batch_size = cbow.shape[0]\n",
    "    \n",
    "    LSTM_network = {}\n",
    "    \n",
    "    Wf, Wi, Wa, Wo, Wy = parameters['Wf'], parameters['Wi'], parameters['Wa'], parameters['Wo'], parameters['Wy']\n",
    "    bf, bi, ba, bo, by = parameters['bf'], parameters['bi'], parameters['ba'], parameters['bo'], parameters['by']\n",
    "\n",
    "    T = sentence_size #Number of LSTM cells? This is also the size of embedding..?\n",
    "    network = initialize_network(hidden_size,T,batch_size)\n",
    "    a,c,f,i,h = network['a'], network['c'],network['f'],network['i'],network['h']\n",
    "    \n",
    "    \n",
    "    #Loop through each word in x. \n",
    "    for t in range(T):\n",
    "        if t>0:\n",
    "            h_prev, c_prev = h[t-1], c[t-1]\n",
    "        else:\n",
    "            h_prev, c_prev = np.zeros((batch_size,hidden_size)), np.zeros((batch_size,hidden_size))\n",
    "            # Concatenate old state with current input\n",
    "        \n",
    "        #Need to specify xt\n",
    "        index_t = index_matrix[:,t] #the t-th word of all the sentences in the batch.  \n",
    "        X_one_hot = np.zeros((batch_size,D))\n",
    "        \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            X_one_hot[i,int(index_t[i])] = 1.\n",
    "            \n",
    "        X = np.column_stack((h_prev, X_one_hot))\n",
    "\n",
    "        f[t] = sigmoid(X @ Wf + bf)\n",
    "        print(f[t].shape)\n",
    "        i[t] = sigmoid(X @ Wi + bi)\n",
    "        o[t] = sigmoid(X @ Wo + bo)\n",
    "        a[t] = tanh(X @ Wa + ba)\n",
    "\n",
    "        c[t] = hf * c_prev + hi * a\n",
    "        h[t] = ho * tanh(c)\n",
    "\n",
    "    y_hat = h[T-1] @ Wy + by\n",
    "    prob = softmax(y_hat) #Probably not softmax, as the scores are numerical. \n",
    "\n",
    "    #previous_outputs = [h, c] # Cache the states of current h & c for next iter\n",
    "    cache = {} # Add all intermediate variables to this cache\n",
    "\n",
    "    layers_name = ['x','a','i','f','c','h','y_hat']\n",
    "    layers = [x,a,i,f,c,h,y_hat]\n",
    "    \n",
    "    for i in range(len(layers_name)):\n",
    "        LSTM_network[layers_name[i]] = layers[i]\n",
    "    \n",
    "    return LSTM_network\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def MSE(y,y_hat):\n",
    "    return (y - y_hat).T @ (y - y_hat)/(1.0*len(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need to find a way to extract f_next... maybe no need\n",
    "#backward for one particular layer t. \n",
    "#Do i update previous_outputs here with current h and c? \n",
    "\n",
    "def LSTM_backward(network,y,parameters, cost_func='MSE',sentence_size=30):\n",
    "    #X is just a particular input X_t\n",
    "    x_arr,y_hat, a_arr,c_arr,f_arr,h_arr,i_arr,o_arr= network['x'],network['y_hat'],network['a'],network['c'],network['f'],network['h'],network['i'],network['o']\n",
    "    \n",
    "    Wf, Wi, Wa, Wo, Wy = parameters['Wf'], parameters['Wi'], parameters['Wa'], parameters['Wo'], parameters['Wy']\n",
    "    bf, bi, ba, bo, by = parameters['bf'], parameters['bi'], parameters['bc'], parameters['bo'], parameters['by']\n",
    "    \n",
    "    hidden_size = a.shape[0]\n",
    "    \n",
    "    grads = initialize_grads(input_size,hidden_size)\n",
    "    grads_next_outputs = [np.zeros(hidden_size),np.zeros(hidden_size)]\n",
    "    \n",
    "    #T is the number of words in a sentence. size of embedding\n",
    "    T = sentence_size\n",
    "    \n",
    "    if cost_func=='MSE':\n",
    "        d_y = y - y_hat \n",
    "        \n",
    "    d_Wy = h_arr.T @ d_y\n",
    "        \n",
    "    d_hT = d_y @ Wy.T\n",
    "    \n",
    "    for t in range(T)[::-1]:\n",
    "        x,a,c,f,h,i,o = x_arr[t],a_arr[t],c_arr[t],f_arr[t],h_arr[t],i_arr[t],o_arr[t]\n",
    "        \n",
    "        if t>0:\n",
    "            h_prev = h_arr[t-1]\n",
    "            c_prev = c_prev[t-1]\n",
    "        else:\n",
    "            h_prev = np.zeros(hidden_size)\n",
    "            c_prev = np.zeros(hidden_size)\n",
    "        \n",
    "        #Gradients of h and c*f stored for t+1\n",
    "        d_h_next, d_c_next_times_f = grads_next_outputs[0],grads_next_outputs[1]\n",
    "        \n",
    "        inputs = [h_prev,x]\n",
    "        \n",
    "        d_h =  d_hT + d_h_next #### what is the dh thingie omg\n",
    "        d_c =  o * d_h * dtanh(c) + d_c_next_times_f\n",
    "        d_a =  d_c * i\n",
    "        d_i =  d_c * a \n",
    "        d_f =  d_c * c_prev\n",
    "        d_o = d_h * np.tanh(c) \n",
    "\n",
    "        #Stack those values together ...\n",
    "        d_gates = [d_a*(1-a**2), d_i*i*(1-i),d_f*f*(1-f), d_o*o*(1-o)] #gradient for pre-non-linear for the gates\n",
    "        W_gates =  [Wa, Wi, Wf, Wo]   \n",
    "        d_inputs = sum[(W_gates[i].T * d_gates[i] for i in range(4))] #d_X contains both gradient for x and h_t-1. \n",
    "\n",
    "        #X is the input, X and h_prev\n",
    "        #Do we only need to update the weights and biases? \n",
    "\n",
    "        d_Wa += inputs @ d_gates[0].T\n",
    "        d_Wi += inputs @ d_gates[1].T\n",
    "        d_Wf += inputs @ d_gates[2].T\n",
    "        d_Wo = inputs @ d_gates[3].T\n",
    "\n",
    "        d_ba += d_gates[0]\n",
    "        d_bi += d_gates[1]\n",
    "        d_bf += d_gates[2]\n",
    "        d_bo += d_gates[3]    \n",
    "\n",
    "        # Split the concatenated X, so that we get our gradient of h_prev\n",
    "        d_h_next = d_inputs[:hidden_size, :] #Need to know how this is extracted exactly\n",
    "        # Gradient for c_old in c = f * c_prev + hi * a, this case, c_prev becomes current c\n",
    "        d_c_next_times_f = f * dc \n",
    "\n",
    "        grads_next_outputs[0] = d_h_next\n",
    "        grads_next_outputs[1] = d_c_next_times_f \n",
    "\n",
    "        #previous_outputs[0] = h\n",
    "        #previous_outputs[1] = c\n",
    "        \n",
    "        grads = {'Wa':d_Wa,'Wi':d_Wi,'Wf':d_Wf,'Wo':d_Wo,'ba':d_ba,'bi':d_bi,'bf':d_bf,'bo':d_bo}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    param_list = ['Wf','Wi','Wa','Wo','Wy','bf','bi','ba','bo','by']\n",
    "    for param in param_list:  \n",
    "        parameters[param] = parameters[param] - learning_rate * grads[param]\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations,x,y,learning_rate = 0.001,minibatch_size = 100, epsilon=1*10^(-3)):\n",
    "    l = x.shape[0]\n",
    "    n = x.shape[1]\n",
    "    parameters = initialize_parameters()\n",
    "    num_batches = int(l/minibatch_size) \n",
    "    batch_costs = np.zeros(num_batches) \n",
    "    average_costs = np.zeros(iterations) \n",
    "    prev_average_cost = 20000 \n",
    "    i=1\n",
    "\n",
    "    while (cost_diff> epsilon) & (i< iterations):\n",
    "        startTime = datetime.now()\n",
    "        #np.random.shuffle(x_train) #Shuffle the x_train array at each iteration\n",
    "        for k in range(0, l, minibatch_size):\n",
    "            batch_num = k/(minibatch_size)\n",
    "            x_mini = x[k:k + minibatch_size]\n",
    "            y_mini = y[k:k + minibatch_size]\n",
    "        #model = initialize_network(h_size,T)\n",
    "        grads = initialize_grads(input_size,h_size)\n",
    "        model = LSTM_forward(x_mini,y_mini,parameters)\n",
    "        loss = MSE(y_mini,model['y_hat'])\n",
    "        grads = LSTM_backward(model,y_mini,parameters, cost_func='MSE')\n",
    "        \n",
    "    return model,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a basic error analysis. What does the model predict well and what does it do less well on? Is there anything you can take away which could be used to improve the model in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are expected to write a short (less than 400 words in total) report on: \n",
    "\n",
    "1.Data analysis - your key insights\n",
    "\n",
    "2.Model decisions and tuning - RNN\n",
    "\n",
    "3.Error analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
