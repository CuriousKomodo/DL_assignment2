{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/kefei/Documents/DSML/Introduction to Deep Learning/Second assignment/comp0090_1819-master/assignment_2/2_2_rnn_sequence_enconding/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    text_file = open(path, \"r\")\n",
    "    lines = text_file.readlines()\n",
    "    \n",
    "    x, y = [], []\n",
    "    for line in lines:\n",
    "        line_tokens = line.split(\" \")\n",
    "        y.append(int(line_tokens[0].replace(\"(\",\"\")))\n",
    "\n",
    "        word_token_list = []\n",
    "        for token in line_tokens:\n",
    "            if token.endswith(')'):\n",
    "                word_token_list.append(token.replace(')',''))\n",
    "        x.append(word_token_list)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y = read_data(path+\"train.sexp\");\n",
    "dev_x,   dev_y   = read_data(path+\"dev.sexp\");\n",
    "test_x,  test_y  = read_data(path+\"test.sexp\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "#                             tokenizer = callable,    \\\n",
    "#                             preprocessor = None, \\\n",
    "#                             stop_words = None) \n",
    "#train_data_features = vectorizer.fit_transform(train_x)\n",
    "#train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only consider the top 1000 most popular words. \n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False) #max_features=1000\n",
    "train_x2 = train_x[1:1000]\n",
    "cbow=vectorizer.fit_transform(train_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = cbow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 5096)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make a matrix that stores the indices of each sentence up to 30 words. \n",
    "embedding = 30\n",
    "index_matrix = np.zeros((len(train_x2),embedding))\n",
    "for i in range(len(train_x2)):\n",
    "    for j in range(min(len(train_x2[i]),embedding)):\n",
    "        word = train_x2[i][j]\n",
    "        index_matrix[i,j] = vectorizer.vocabulary_[str(word)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.zeros(len(train_x))\n",
    "for i in range(len(train_x)):\n",
    "    lengths[i] = len(train_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.0"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(train_y[1:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are expected to provide an analysis of the provided data. This can include word frequency analysis by class, character-level frequency analysis, analysis based on manual features (e.g. stopwords, negation words, punctuation), etc. Try to include some creative visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Recurrent Neural Network (RNN) to encode the sequence and predict each of the 5 classes. Tune hyper-parameters on the dev set. Evaluate performance in terms of accuracy on the test set.Bonus question: Compare performance on the train/dev/test sets - what does this tell you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful link on dimensions of LSTM, https://www.quora.com/In-LSTM-how-do-you-figure-out-what-size-the-weights-are-supposed-to-be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization + Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters =initialize_parameters(x)\n",
    "y = transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_network = LSTM_forward(x,y,cbow,parameters,word_dict,index_matrix,batch_size=None,sentence_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size=128\n",
    "T=30\n",
    "batch_size = 8544\n",
    "\n",
    "network = initialize_network(hidden_size,T,batch_size)\n",
    "a,c,f,i,h = network['a'], network['c'],network['f'],network['i'],network['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(y):\n",
    "    y_one_hot = np.zeros((len(y),5))\n",
    "    for i in range(len(y)):\n",
    "        correct_class = int(y[i])\n",
    "        y_one_hot[i,correct_class]=1.0\n",
    "    return y_one_hot\n",
    "\n",
    "def initialize_parameters(x):\n",
    "    hidden_size = 128 # Number of LSTM layer's neurons, a,f,i,o\n",
    "    n = x.shape[1] # Number of input dimension == number of items in vocabulary\n",
    "    W_size = hidden_size + n # Because we will concatenate LSTM state with the input\n",
    "    \n",
    "    \n",
    "    #Maybe will concatenate Wf,Wi,Wc and Wo together to multiply in parallel\n",
    "    Wf = np.random.randn(W_size, hidden_size) / np.sqrt( W_size/ 2.)\n",
    "    Wi = np.random.randn(W_size, hidden_size) / np.sqrt( W_size/ 2.)\n",
    "    Wa = np.random.randn(W_size, hidden_size) / np.sqrt( W_size/ 2.)\n",
    "    Wo = np.random.randn(W_size, hidden_size) / np.sqrt( W_size/ 2.)\n",
    "    Wy = np.random.randn(hidden_size, 5) / np.sqrt(n / 2.)\n",
    "    \n",
    "    bf=np.zeros((1, hidden_size))\n",
    "    bi=np.zeros((1, hidden_size))\n",
    "    ba=np.zeros((1, hidden_size))\n",
    "    bo=np.zeros((1, hidden_size))\n",
    "    by=np.zeros((1, 5))\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['Wf'] = Wf\n",
    "    parameters['bf'] = bf\n",
    "    parameters['Wi'] = Wi\n",
    "    parameters['bi'] = bi\n",
    "    parameters['Wa'] = Wa\n",
    "    parameters['ba'] = ba\n",
    "    parameters['Wo'] = Wo\n",
    "    parameters['bo'] = bo\n",
    "    parameters['Wy'] = Wy\n",
    "    parameters['by'] = by\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def initialize_network(hidden_size,T,batch_size):\n",
    "    f = np.zeros((T,batch_size,hidden_size))\n",
    "    i = np.zeros((T,batch_size,hidden_size))\n",
    "    a = np.zeros((T,batch_size,hidden_size))\n",
    "    o = np.zeros((T,batch_size,hidden_size))\n",
    "    c = np.zeros((T,batch_size,hidden_size))\n",
    "    h = np.zeros((T,batch_size,hidden_size))\n",
    "    \n",
    "    LSTM_network = {'f':f,'i':i,'a':a,'o':o,'c':c,'h':h}\n",
    "    return LSTM_network\n",
    "\n",
    "\n",
    "def initialize_grads(input_size,h_size):\n",
    "    d_Wa = np.zeros((input_size, h_size))\n",
    "    d_Wi = np.zeros((input_size, h_size))\n",
    "    d_Wf = np.zeros((input_size, h_size))\n",
    "    d_Wo = np.zeros((input_size, h_size))\n",
    "\n",
    "    d_ba = np.zeros(h_size)\n",
    "    d_bi = np.zeros(h_size)\n",
    "    d_bf = np.zeros(h_size)\n",
    "    d_bo = np.zeros(h_size)\n",
    "    \n",
    "    grads = {'Wa':d_Wa,'Wi':d_Wi,'Wf':d_Wf,'Wo':d_Wo,'ba':d_ba,'bi':d_bi,'bf':d_bf, 'bo':d_bo}\n",
    "    return grads\n",
    "\n",
    "\n",
    "#Input one current X or one sentence? \n",
    "#index matrix need to be adjusted with batch! \n",
    "def LSTM_forward(x,y,cbow,parameters,word_dict,index_matrix,batch_size=None,sentence_size=30):\n",
    "    hidden_size=128\n",
    "    D = cbow.shape[1]\n",
    "    if not batch_size:\n",
    "        batch_size = cbow.shape[0]\n",
    "    \n",
    "    LSTM_network = {}\n",
    "    \n",
    "    Wf, Wi, Wa, Wo, Wy = parameters['Wf'], parameters['Wi'], parameters['Wa'], parameters['Wo'], parameters['Wy']\n",
    "    bf, bi, ba, bo, by = parameters['bf'], parameters['bi'], parameters['ba'], parameters['bo'], parameters['by']\n",
    "\n",
    "    T = sentence_size #Number of LSTM cells? This is also the size of embedding..?\n",
    "    network = initialize_network(hidden_size,T,batch_size)\n",
    "    a,c,f,i,o,h = network['a'], network['c'],network['f'],network['i'],network['o'],network['h']\n",
    "    \n",
    "    \n",
    "    #Loop through each word in x. \n",
    "    for t in range(T):\n",
    "        if t>0:\n",
    "            h_prev, c_prev = h[t-1], c[t-1]\n",
    "        else:\n",
    "            h_prev, c_prev = np.zeros((batch_size,hidden_size)), np.zeros((batch_size,hidden_size))\n",
    "            # Concatenate old state with current input\n",
    "        \n",
    "        #Need to specify xt\n",
    "        index_t = index_matrix[:,t] #the t-th word of all the sentences in the batch.  \n",
    "        X_one_hot = np.zeros((batch_size,D))\n",
    "        \n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            X_one_hot[b,int(index_t[b])] = 1.\n",
    "            \n",
    "        X = np.column_stack((h_prev, X_one_hot))\n",
    "        \n",
    "        f[t] = sigmoid(X @ Wf + bf)\n",
    "        i[t] = sigmoid(X @ Wi + bi)\n",
    "        o[t] = sigmoid(X @ Wo + bo)\n",
    "        a[t] = tanh(X @ Wa + ba)\n",
    "\n",
    "        c[t] = f[t] * c_prev + i[t] * a[t]\n",
    "        h[t] = o[t] * tanh(c[t])\n",
    "        \n",
    "    y_hat = h[T-1] @ Wy + by\n",
    "    pred = softmax(y_hat) #Probably not softmax, as the scores are numerical. \n",
    "\n",
    "    #previous_outputs = [h, c] # Cache the states of current h & c for next iter\n",
    "    cache = {} # Add all intermediate variables to this cache, maybe will be more efficient. \n",
    "\n",
    "    layers_name = ['a','i','f','o','c','h','y_hat','pred']\n",
    "    layers = [a,i,f,o,c,h,y_hat,pred]\n",
    "    \n",
    "    for i in range(len(layers_name)):\n",
    "        LSTM_network[layers_name[i]] = layers[i]\n",
    "    \n",
    "    return LSTM_network\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def MSE(y,y_hat):\n",
    "    return (y - y_hat).T @ (y - y_hat)/(1.0*len(y))\n",
    "\n",
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp/sum(exp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful link, https://github.com/mukul-rathi/blogPost-tutorials/blob/master/RecurrentNeuralNet/LSTM.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a clear illustration of gradient through time, https://arxiv.org/pdf/1503.04069.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe..? https://github.com/mukul-rathi/blogPost-tutorials/blob/master/RecurrentNeuralNet/LSTM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = LSTM_backward(LSTM_network,y,index_matrix,parameters, cost_func='CE',sentence_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to find a way to extract f_next... maybe no need\n",
    "#backward for one particular layer t. \n",
    "#Do i update previous_outputs here with current h and c? \n",
    "\n",
    "def LSTM_backward(network,y,index_matrix,parameters, batch_size=None,cost_func='CE',sentence_size=30):\n",
    "    #X is just a particular input X_t\n",
    "        \n",
    "    pred, a_arr,c_arr,f_arr,h_arr,i_arr,o_arr= network['pred'],network['a'],network['c'],network['f'],network['h'],network['i'],network['o']\n",
    "    \n",
    "    Wf, Wi, Wa, Wo, Wy = parameters['Wf'], parameters['Wi'], parameters['Wa'], parameters['Wo'], parameters['Wy']\n",
    "    bf, bi, ba, bo, by = parameters['bf'], parameters['bi'], parameters['ba'], parameters['bo'], parameters['by']\n",
    "    \n",
    "    if not batch_size:\n",
    "        batch_size = cbow.shape[0]\n",
    "        \n",
    "    input_size = Wf.shape[0]\n",
    "    hidden_size = Wf.shape[1]\n",
    "    \n",
    "    grads = initialize_grads(input_size,hidden_size)\n",
    "    d_Wa,d_Wi,d_Wf,d_Wo,d_ba,d_bi,d_bf,d_bo = grads['Wa'],grads['Wi'],grads['Wf'],grads['Wo'],grads['ba'],grads['bi'],grads['bf'],grads['bo']\n",
    "    grads_next_outputs = [np.zeros(hidden_size),np.zeros(hidden_size)]\n",
    "    \n",
    "    #T is the number of words in a sentence. size of embedding\n",
    "    T = sentence_size\n",
    "    D = int(index_matrix.max() + 1)\n",
    "    \n",
    "    #https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function\n",
    "    if cost_func=='CE': \n",
    "        d_yhat = pred -y \n",
    "        \n",
    "    d_Wy = h_arr[-1].T @ d_yhat\n",
    "        \n",
    "    d_hT = d_yhat @ Wy.T\n",
    "    \n",
    "    print('d_ht shape',d_hT.shape)\n",
    "    \n",
    "    for t in range(T)[::-1]:\n",
    "        index_t = index_matrix[:,t] #the t-th word of all the sentences in the batch.  \n",
    "        \n",
    "        X_one_hot = np.zeros((batch_size,D))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            X_one_hot[b,int(index_t[b])] = 1.\n",
    "        \n",
    "        a,c,f,h,i,o = a_arr[t],c_arr[t],f_arr[t],h_arr[t],i_arr[t],o_arr[t]\n",
    "        \n",
    "        if t>0:\n",
    "            h_prev = h_arr[t-1]\n",
    "            c_prev = c_arr[t-1]\n",
    "        else:\n",
    "            h_prev = np.zeros(hidden_size)\n",
    "            c_prev = np.zeros(hidden_size)\n",
    "        \n",
    "        #Gradients of h and c*f stored for t+1\n",
    "        d_h_next, d_c_next_times_f = grads_next_outputs[0],grads_next_outputs[1]\n",
    "        \n",
    "        inputs = np.column_stack((h_prev, X_one_hot))\n",
    "        \n",
    "        d_h =  d_hT + d_h_next #### what is the dh thingie omg\n",
    "        d_c =  o * d_h * dtanh(c) + d_c_next_times_f\n",
    "        d_a =  d_c * i\n",
    "        d_i =  d_c * a \n",
    "        d_f =  d_c * c_prev\n",
    "        d_o = d_h * np.tanh(c) \n",
    "\n",
    "        #Stack those values together ...\n",
    "        d_gates = [d_a*(1-a**2), d_i*i*(1-i),d_f*f*(1-f), d_o*o*(1-o)] #gradient for pre-non-linear for the gates\n",
    "        W_gates =  [Wa, Wi, Wf, Wo]   \n",
    "        d_inputs = sum((d_gates[i]@W_gates[i].T for i in range(4))) #d_X contains both gradient for x and h_t-1. \n",
    "\n",
    "        #X is the input, X and h_prev\n",
    "        #Do we only need to update the weights and biases? \n",
    "\n",
    "        d_Wa += inputs.T @ d_gates[0]\n",
    "        d_Wi += inputs.T @ d_gates[1]\n",
    "        d_Wf += inputs.T @ d_gates[2]\n",
    "        d_Wo = inputs.T @ d_gates[3]\n",
    "        \n",
    "        #A=np.sum(d_gates[0],axis=0, keepdims=False) \n",
    "        #print(A.shape)\n",
    "        d_ba += np.sum(d_gates[0],axis=0, keepdims=False) \n",
    "        d_bi += np.sum(d_gates[1],axis=0, keepdims=False) \n",
    "        d_bf += np.sum(d_gates[2],axis=0, keepdims=False) \n",
    "        d_bo += np.sum(d_gates[3],axis=0, keepdims=False) \n",
    "        \n",
    "        print('d_input shape',d_inputs.shape)\n",
    "        # Split the concatenated X, so that we get our gradient of h_prev\n",
    "        d_h_next = d_inputs[:, :hidden_size] #Need to know how this is extracted exactly\n",
    "        print('d_h_next shape',d_h_next.shape)\n",
    "        # Gradient for c_old in c = f * c_prev + hi * a, this case, c_prev becomes current c\n",
    "        d_c_next_times_f = f * d_c \n",
    "\n",
    "        grads_next_outputs[0] = d_h_next\n",
    "        grads_next_outputs[1] = d_c_next_times_f \n",
    "\n",
    "    d_ba  = np.sum(d_ba,axis=1, keepdims=True) \n",
    "    d_bi  = np.sum(d_bi,axis=1, keepdims=True) \n",
    "    d_bf  = np.sum(d_bf,axis=1, keepdims=True) \n",
    "    d_bo  = np.sum(d_bo,axis=1, keepdims=True) \n",
    "    \n",
    "    grads = {'Wa':d_Wa,'Wi':d_Wi,'Wf':d_Wf,'Wo':d_Wo,'ba':d_ba,'bi':d_bi,'bf':d_bf,'bo':d_bo}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    param_list = ['Wf','Wi','Wa','Wo','Wy','bf','bi','ba','bo','by']\n",
    "    for param in param_list:  \n",
    "        parameters[param] = parameters[param] - learning_rate * grads[param]\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations,x,y,learning_rate = 0.001,minibatch_size = 100, epsilon=1*10^(-3)):\n",
    "    l = x.shape[0]\n",
    "    n = x.shape[1]\n",
    "    parameters = initialize_parameters()\n",
    "    num_batches = int(l/minibatch_size) \n",
    "    batch_costs = np.zeros(num_batches) \n",
    "    average_costs = np.zeros(iterations) \n",
    "    prev_average_cost = 20000 \n",
    "    i=1\n",
    "\n",
    "    while (cost_diff> epsilon) & (i< iterations):\n",
    "        startTime = datetime.now()\n",
    "        #np.random.shuffle(x_train) #Shuffle the x_train array at each iteration\n",
    "        for k in range(0, l, minibatch_size):\n",
    "            batch_num = k/(minibatch_size)\n",
    "            x_mini = x[k:k + minibatch_size]\n",
    "            y_mini = y[k:k + minibatch_size]\n",
    "        #model = initialize_network(h_size,T)\n",
    "        grads = initialize_grads(input_size,h_size)\n",
    "        model = LSTM_forward(x_mini,y_mini,parameters)\n",
    "        #loss = CE(y_mini,model['y_hat'])\n",
    "        grads = LSTM_backward(model,y_mini,parameters, cost_func='CE')\n",
    "        ### Do we use Adam? \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "    return model,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/navjindervirdee/lstm-neural-network-from-scratch/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Need to edit this as it is just a reference from online. \n",
    "def loss_function(y_pred,y, parameters, lambd):\n",
    "    m = y.shape[1]\n",
    "    cost = (-1/m)*np.sum(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "    \n",
    "    regularisation_term = 0\n",
    "    for key in parameters:\n",
    "        if \"W\" in key:\n",
    "            regularisation_term += np.sum(np.square(parameters[key]))\n",
    "    \n",
    "    regularised_cost = cost + (lambd/(2*m))*regularisation_term\n",
    "    \n",
    "    return regularised_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Need to edit this as it is just a reference from online. \n",
    "def backprop_checker(parameters,grads, x, y,lambd):\n",
    "    epsilon = 1e-7\n",
    "    rel_threshold = 1e-2\n",
    "    num_sample = 10\n",
    "    flag = True #if backprop is correct\n",
    "    print(\"Checking gradients...\")\n",
    "    for param in parameters.keys(): \n",
    "        print(\"Checking: \" + param)\n",
    "        dims = parameters[param].shape\n",
    "        \n",
    "        \n",
    "        num_grad = 0\n",
    "        backprop_grad = 0\n",
    "        \n",
    "        for _ in range(num_sample): #sample 10 neurons\n",
    "            idx = np.zeros(len(dims))\n",
    "            for i in range(len(dims)):\n",
    "                idx[i] = np.random.randint(0,dims[i])\n",
    "            idx = tuple(idx.astype(int))\n",
    "\n",
    "            parameters[param][idx]= parameters[param][idx] + epsilon\n",
    "            y_pred_plus = forward_prop(x,parameters)[0]\n",
    "            J_plus = loss_function(y_pred_plus,y,parameters,lambd)\n",
    "            parameters[param][idx]= parameters[param][idx] - 2*epsilon\n",
    "\n",
    "            y_pred_minus = forward_prop(x,parameters)[0]\n",
    "\n",
    "            J_minus = loss_function(y_pred_minus,y,parameters,lambd)\n",
    "            parameters[param][idx]= parameters[param][idx]+ epsilon\n",
    "\n",
    "            num_grad += (J_plus-J_minus)/(2*epsilon)\n",
    "            backprop_grad += grads[\"d\"+param][idx]\n",
    "            \n",
    "            \n",
    "        num_grad/=num_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a basic error analysis. What does the model predict well and what does it do less well on? Is there anything you can take away which could be used to improve the model in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are expected to write a short (less than 400 words in total) report on: \n",
    "\n",
    "1.Data analysis - your key insights\n",
    "\n",
    "2.Model decisions and tuning - RNN\n",
    "\n",
    "3.Error analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
